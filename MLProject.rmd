#Weight Lifting Data Analysis & Prediction  
Andrew A. Ritz  
March, 2016

####Introduction
In recent years, various groups have been collecting data from accelerometers and other wearable devices and using that information to analyze people's behaviour and activity while they wear them.  In particular, the [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) project has been collecting and publishing data in order to analyze the efficacy of certain weight lifting exercises.  

This data contains information collected from various sensors attached to the participant's body, while they performed various weight lifting exercises.  The exercises were supervised by a professional, and were rated based on their performance, which is stored in the **classe** variable of the dataset.  For this dataset, class *A* refers to correct execution of the exercise, while the other 4 classes *B-E* indicate some common mistake.  More information about this dataset can be found at the [project's website](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises).


####Purpose & Goals
The purpose of this exercise is to use the set of weight lifting data, to develop a machine learning model that can then be used to make predictions about new data collected from these sensors at a future date.  If our model is successful we could use it to automate the grading of performed exercises, rather than relying on a human professional.  We can also use it to pass the quiz at the the end of the course :)



```{r echo=FALSE}

library(caret)
library(ggplot2)

```


### Data Acquisition
Before we can analyze and build our model, we must acquire the data.  We will be downloading this from the internet at the specified locations below.  We will be acquiring a training set (large) and a test set (small).

```{r echo=FALSE}
# Cleanup..
rm(list=ls())

getData<-function(path, url)
{
  res<-NULL
  if (!file.exists(path))
  {
    res<-download.file(url, path)
  }
  res<-read.csv(path)
  res
}

# Get a copy of the train / test data that we are going to use.
# We use our downloader function so we don't have to grab it each time.
trainData<-getData('pml-training.csv', 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')
testData<-getData('pml-testing.csv', 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')

```


```{r}

```

####Error Prediction
We need to estimate our *out of sample* error rate.  The out of sample error rate is the error rate that we receive on the final testing set.  In order to estimate this, we will break our initial training set into further training and test sets.

```{r}

# The ol' 80/20 split shoud suffice.
ti<-createDataPartition(trainData$classe, p=0.8, list=FALSE)

subTrain <- trainData[ti,]
subTest <- trainData[-ti,]

```



####Feature Selection
Before we begin, we will want to be sure to identify and remove any predictors in our data set that may not contribute to our model.  Fewer predictors will also help to improve our processing time.  Since the data is mostly continuous, we will be identifying and removing those predictors that are mostly NA, have little variance, or have little meaning for our analysis.

```{r}

# We will have to perform these transforms on any other data set that we test against, so a reusable function is the best approach.

# First the predictors with near zero variance.
zeroVars = nearZeroVar(subTrain, saveMetrics = TRUE)$nzv

# Then we wil identify the cols that are mostly NA.
naVars = apply(subTrain, MARGIN = 2, function(x) mean(is.na(x)) > 0.95)
naVars<- unname(naVars)

# Based on the description of the data, we will be dropping the timestamp variables since they won't
# be indicative of how someone performs their exercise.
# We also remove the 'X' predictor since this is just an index.
dropVars = which(names(subTrain) %in% c('X','cvtd_timestamp', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'user_name'))


# We compose our final set of indexes by merging our info from above.  
trimCols<-zeroVars
trimCols[which(naVars)] = TRUE
trimCols[dropVars] = TRUE

#We invert it so that we are sure to keep entries that aren't selected for removal (FALSE)
#This is the approach we will use for the remainder of our data.  May as well set them all up now.
subTrain2 = subTrain[,!trimCols]
subTest2 = subTest[,!trimCols]
test2 = testData[,!trimCols]

```

####Model Training
To train our model, we will be using the Random Forest approach.  The Random Forest is well known for being a very effective machine learning technique, and is appropriate for predicting class labels (factors) which we intend to do in this exercise.  We are going to use the **train** function from the **caret** package to perform this.  We are also going to be using k-fold cross validation which we will configure before the training operation begins.


```{r, cache=TRUE}

# We will use three folds, which should be sufficient.  More folds may increase accuracy, but will lead to much longer processing times.
tc <- trainControl(method="cv", number=3, verboseIter=F)
model <- train(classe~., data=subTrain2, method="rf", trControl=tc)

```

Once we have created our model, we can run a prediciton on our test set, and compute our out of sample error, which is also our estimate for our final test set.
```{r}

# predicted
pv <- predict(model, subTest2)

# logical array of predicted values that match the actual values.
mv <- pv == subTest2$classe

predErr <- 1-(sum(mv) / length(mv))

```


We can see that our computed out of sample error rate is very small, at `r round(predErr*100,2)`%  It appears that we have trained a very accurate model.  Of course, the actual error rate that we receive on our real test set will likely be higher.



####Final Predictions
As a final step, we will use our model to predict class labels for our test observations.  Please note that with this test set, we have no way of reporting an error rate because the set does not contain the *classe* variable.

```{r}

#No classe variable.
message(paste0("Data has 'classe'? ", length(grep("classe",names(test2))) > 0))

final<-predict(model, test2)
finalRate = sum(final == 'A') / length(final)
message("Final Predictions")
final


```

Yikes!  Looks like only `r round(finalRate * 100)`% of the exercises are being performed correctly in our test set if our model is correct.  Maybe we should keep our trainers around a while longer!
